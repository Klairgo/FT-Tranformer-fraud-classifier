{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y62nwlas_3O1",
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# PyTorch FT-Transformer Fraud Classification\n",
        "\n",
        "This notebook implements a grid search for the FT-Transformer model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i0-kYaVz_3O4"
      },
      "outputs": [],
      "source": [
        "# Variables\n",
        "train_dataset_path = '../data/train.csv'\n",
        "test_dataset_path = '../data/test.csv'\n",
        "metadata_path = '../data/preprocessing_metadata.json'\n",
        "class_label = 'Class'\n",
        "random_seed = 42"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OLR5lOXZ_3O5",
        "outputId": "75ee8152-54ad-49a4-a49f-d5a701a15cf0"
      },
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score, f1_score,\n",
        "    confusion_matrix, classification_report, roc_auc_score,\n",
        "    precision_recall_curve, roc_curve, average_precision_score\n",
        ")\n",
        "import pickle\n",
        "import warnings\n",
        "import multiprocessing as mp\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set multiprocessing start method for Jupyter compatibility\n",
        "try:\n",
        "    mp.set_start_method('spawn', force=True)\n",
        "    print(\"Multiprocessing start method set to 'spawn' for Jupyter compatibility\")\n",
        "except RuntimeError:\n",
        "    print(\"Multiprocessing start method already set\")\n",
        "\n",
        "# Disable multiprocessing in DataLoaders for Jupyter safety\n",
        "import os\n",
        "os.environ['PYTORCH_DATALOADER_NUM_WORKERS'] = '0'\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "np.random.seed(random_seed)\n",
        "torch.manual_seed(random_seed)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(random_seed)\n",
        "    torch.cuda.manual_seed_all(random_seed)\n",
        "    # Ensure deterministic behavior on CUDA\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "# Device configuration - prioritize CUDA if available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(\"All libraries imported successfully!\")\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"CUDA memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Set plotting style\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "!pip install rtdl_revisiting_models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jhaHMKsk_3O7",
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 1. Data Loading and Exploration\n",
        "\n",
        "Loading the balanced training data and original test data for fraud detection.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6qlT-yLt_3O7",
        "outputId": "0165c42b-0e23-47d0-f97a-c26fa55cf52e"
      },
      "outputs": [],
      "source": [
        "# Load the datasets\n",
        "print(\"Loading datasets...\")\n",
        "\n",
        "# Load balanced training data (SMOTE applied)\n",
        "train_df = pd.read_csv(train_dataset_path)\n",
        "print(f\"Balanced training data shape: {train_df.shape}\")\n",
        "\n",
        "# Load original test data (imbalanced)\n",
        "test_df = pd.read_csv(test_dataset_path)\n",
        "print(f\"Test data shape: {test_df.shape}\")\n",
        "\n",
        "# Display basic information\n",
        "print(\"\\n=== Training Data Info ===\")\n",
        "print(train_df.info())\n",
        "print(f\"\\nClass distribution in training data:\")\n",
        "print(train_df[class_label].value_counts())\n",
        "print(f\"Training fraud percentage: {train_df[class_label].mean()*100:.2f}%\")\n",
        "\n",
        "print(\"\\n=== Test Data Info ===\")\n",
        "print(f\"\\nClass distribution in test data:\")\n",
        "print(test_df[class_label].value_counts())\n",
        "print(f\"Test fraud percentage: {test_df[class_label].mean()*100:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R8EbMEzC_3O9",
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 2. Data Preprocessing for FT-Transformer\n",
        "\n",
        "Preparing the data for TabNet training including feature separation and encoding.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e2Vbut1C_3O9",
        "outputId": "2ff66c21-130f-4ce0-a593-9adc6ffb7f2f"
      },
      "outputs": [],
      "source": [
        "# Data preprocessing for TabNet\n",
        "def preprocess_data(train_df, test_df):\n",
        "    \"\"\"\n",
        "    Preprocess data for TabNet training\n",
        "\n",
        "    Returns:\n",
        "        X_train, y_train, X_test, y_test, cat_idxs, cat_dims\n",
        "    \"\"\"\n",
        "    import json\n",
        "\n",
        "    with open(metadata_path, 'r') as f:\n",
        "        metadata = json.load(f)\n",
        "    print(f\"Loaded metadata: {metadata}\")\n",
        "\n",
        "    # Separate features and target\n",
        "    feature_cols = [col for col in train_df.columns if col != class_label]\n",
        "\n",
        "    X_train = train_df[feature_cols].copy()\n",
        "    y_train = train_df[class_label].values.astype(int)\n",
        "\n",
        "    X_test = test_df[feature_cols].copy()\n",
        "    y_test = test_df[class_label].values.astype(int)\n",
        "\n",
        "    print(f\"Feature columns: {len(feature_cols)}\")\n",
        "    print(f\"Training samples: {len(X_train)}\")\n",
        "    print(f\"Test samples: {len(X_test)}\")\n",
        "\n",
        "    categorical_cols = []\n",
        "    numerical_cols = [col for col in feature_cols if col not in categorical_cols]\n",
        "\n",
        "\n",
        "    # Get categorical info from metadata but verify against current feature order\n",
        "    metadata_features = metadata.get('feature_columns', [])\n",
        "\n",
        "    print(f\"Metadata feature order: {metadata_features}\")\n",
        "    print(f\"Current feature order: {feature_cols}\")\n",
        "\n",
        "    # Convert DataFrames to numpy arrays for TabNet (required)\n",
        "    X_train = X_train.values.astype(np.float32)\n",
        "    X_test = X_test.values.astype(np.float32)\n",
        "\n",
        "    print(f\"\\nConverted to numpy arrays for TabNet compatibility\")\n",
        "    print(f\"X_train type: {type(X_train)}, shape: {X_train.shape}\")\n",
        "    print(f\"X_test type: {type(X_test)}, shape: {X_test.shape}\")\n",
        "\n",
        "    return X_train, y_train, X_test, y_test, feature_cols, numerical_cols\n",
        "\n",
        "# Preprocess the data\n",
        "X_train, y_train, X_test, y_test, feature_names, numerical_cols = preprocess_data(train_df, test_df)\n",
        "\n",
        "print(f\"\\nFinal shapes:\")\n",
        "print(f\"X_train: {X_train.shape}\")\n",
        "print(f\"y_train: {y_train.shape}\")\n",
        "print(f\"X_test: {X_test.shape}\")\n",
        "print(f\"y_test: {y_test.shape}\")\n",
        "print(f\"Class distribution in y_train: {np.bincount(y_train)}\")\n",
        "print(f\"Class distribution in y_test: {np.bincount(y_test)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qa6uIaSw_3O-",
        "outputId": "d1d1e12b-8ca8-4829-a017-cd6df4880466"
      },
      "outputs": [],
      "source": [
        "# Create validation split from training data\n",
        "X_train_df = pd.DataFrame(X_train, columns=feature_names)\n",
        "y_train_df = pd.DataFrame(y_train, columns=[class_label])\n",
        "\n",
        "X_train_split, X_val, y_train_split, y_val = train_test_split(\n",
        "    X_train_df, y_train_df,\n",
        "    test_size=0.1,\n",
        "    random_state=random_seed,\n",
        "    stratify=y_train_df\n",
        ")\n",
        "\n",
        "print(f\"Training split: {X_train_split.shape}\")\n",
        "print(f\"Validation split: {X_val.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "veIgrYd7_3PA",
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 3. FT-Transformer Model Definition and Training\n",
        "\n",
        "Building and training the TabNet classifier with optimized hyperparameters for fraud detection.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LEutgniy_3PB",
        "outputId": "8be1f6a3-9452-4c38-8a3c-422481414cdd"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from rtdl_revisiting_models import FTTransformer\n",
        "import copy\n",
        "import random\n",
        "\n",
        "class FraudFTTransformer:\n",
        "    def __init__(self, n_num_features, n_cat_features, n_cont_features, cat_cardinalities,\n",
        "                 device=None, seed: int = 42, **model_params):\n",
        "        # Use global device if not specified\n",
        "        if device is None:\n",
        "            device = globals().get('device', torch.device('cuda' if torch.cuda.is_available() else 'cpu'))\n",
        "\n",
        "        self.device = device\n",
        "        self.seed = seed\n",
        "\n",
        "        # Set seeds for reproducibility across libs\n",
        "        random.seed(seed)\n",
        "        np.random.seed(seed)\n",
        "        torch.manual_seed(seed)\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.manual_seed_all(seed)\n",
        "            torch.backends.cudnn.deterministic = True\n",
        "            torch.backends.cudnn.benchmark = False\n",
        "\n",
        "        print(f\"Initializing FraudFTTransformer on device: {self.device}\")\n",
        "\n",
        "        default_params = {\n",
        "            'n_blocks':3,\n",
        "            'd_block': 264,\n",
        "            'attention_n_heads': 12,\n",
        "            'attention_dropout': 0.1,\n",
        "            'ffn_d_hidden': None,\n",
        "            'ffn_d_hidden_multiplier': 5 / 4,\n",
        "            'ffn_dropout': 0.1,\n",
        "            'residual_dropout': 0.0,\n",
        "        }\n",
        "\n",
        "        # Update with provided parameters\n",
        "        default_params.update(model_params)\n",
        "        self.model_params = default_params\n",
        "\n",
        "        print(f\"Model parameters: {self.model_params}\")\n",
        "\n",
        "        # Model configuration optimized for fraud detection\n",
        "        self.model = FTTransformer(\n",
        "            n_cont_features=n_cont_features,\n",
        "            cat_cardinalities=cat_cardinalities,\n",
        "            d_out=2,\n",
        "            **default_params\n",
        "        ).to(self.device)\n",
        "\n",
        "        fraud_count = (y_train_split == 1).sum()\n",
        "        non_fraud_count = (y_train_split == 0).sum()\n",
        "\n",
        "        self.criterion = nn.CrossEntropyLoss().to(self.device)\n",
        "\n",
        "        self.optimizer = optim.AdamW(\n",
        "            self.model.parameters(),\n",
        "            lr=0.0003,\n",
        "            weight_decay=1e-3\n",
        "        )\n",
        "\n",
        "        # Ensure DataLoader worker determinism - use CPU generator for reproducibility\n",
        "        self.generator = torch.Generator().manual_seed(self.seed)\n",
        "\n",
        "        self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "            self.optimizer, mode='min', patience=5, factor=0.7, min_lr=1e-6\n",
        "        )\n",
        "\n",
        "        # Print model info\n",
        "        total_params = sum(p.numel() for p in self.model.parameters())\n",
        "        trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)\n",
        "        print(f\"Model parameters: {total_params:,} total, {trainable_params:,} trainable\")\n",
        "\n",
        "        if torch.cuda.is_available() and self.device.type == 'cuda':\n",
        "            print(f\"GPU memory allocated: {torch.cuda.memory_allocated(self.device) / 1024**2:.1f} MB\")\n",
        "\n",
        "\n",
        "    def fit(self, X_num, y, X_val_num=None, y_val=None,\n",
        "            epochs=100, batch_size=512, patience=20):\n",
        "\n",
        "        X_cat = None\n",
        "\n",
        "        pin_memory = torch.cuda.is_available() and self.device.type == 'cuda'\n",
        "        # Use num_workers=0 in Jupyter to avoid multiprocessing issues\n",
        "        num_workers = 0\n",
        "\n",
        "        print(f\"DataLoader config: pin_memory={pin_memory}, num_workers={num_workers}\")\n",
        "\n",
        "        # Create data loaders with CUDA optimizations\n",
        "        train_dataset = TensorDataset(\n",
        "            torch.FloatTensor(X_num),\n",
        "            torch.LongTensor(y)\n",
        "        )\n",
        "\n",
        "        train_loader = DataLoader(\n",
        "            train_dataset,\n",
        "            batch_size=batch_size,\n",
        "            shuffle=True,\n",
        "            generator=self.generator,\n",
        "            pin_memory=pin_memory,\n",
        "            num_workers=num_workers\n",
        "        )\n",
        "\n",
        "        if X_val_num is not None:\n",
        "            val_dataset = TensorDataset(\n",
        "                torch.FloatTensor(X_val_num),\n",
        "                torch.LongTensor(y_val)\n",
        "            )\n",
        "\n",
        "            val_loader = DataLoader(\n",
        "                val_dataset,\n",
        "                batch_size=batch_size,\n",
        "                pin_memory=pin_memory,\n",
        "                num_workers=num_workers\n",
        "            )\n",
        "\n",
        "        # Training loop\n",
        "        best_val_loss = float('inf')\n",
        "        patience_counter = 0\n",
        "        history = {'train_loss': [], 'train_accuracy': [], 'val_loss': [], 'val_auc': [], 'val_accuracy': []}\n",
        "\n",
        "        print(f\"Starting training for {epochs} epochs on {self.device}\")\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            # Training phase\n",
        "            self.model.train()\n",
        "            train_loss = 0\n",
        "\n",
        "            train_correct = 0\n",
        "            train_total = 0\n",
        "\n",
        "            for batch_idx, (batch_num, batch_y) in enumerate(train_loader):\n",
        "                # Move tensors to device with non_blocking for CUDA efficiency\n",
        "                batch_num = batch_num.to(self.device, non_blocking=True)\n",
        "                batch_y = batch_y.to(self.device, non_blocking=True)\n",
        "\n",
        "                self.optimizer.zero_grad()\n",
        "\n",
        "                # Forward pass\n",
        "                outputs = self.model(batch_num, None)\n",
        "                loss = self.criterion(outputs, batch_y.squeeze())\n",
        "\n",
        "                # Backward pass\n",
        "                loss.backward()\n",
        "\n",
        "                # Gradient clipping for stability\n",
        "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n",
        "\n",
        "                self.optimizer.step()\n",
        "                train_loss += loss.item()\n",
        "\n",
        "                # Calculate training accuracy\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                train_total += batch_y.size(0)\n",
        "                train_correct += (predicted == batch_y.squeeze()).sum().item()\n",
        "\n",
        "                # Clear cache periodically on CUDA to prevent memory issues\n",
        "                if torch.cuda.is_available() and batch_idx % 100 == 0:\n",
        "                    torch.cuda.empty_cache()\n",
        "\n",
        "            # Calculate and store training accuracy\n",
        "            train_accuracy = train_correct / train_total\n",
        "            history['train_accuracy'].append(train_accuracy)\n",
        "\n",
        "            # Validation phase\n",
        "            if X_val_num is not None:\n",
        "                val_loss, val_auc, val_accuracy = self._validate(val_loader)\n",
        "                history['val_loss'].append(val_loss)\n",
        "                history['val_auc'].append(val_auc)\n",
        "                history['val_accuracy'].append(val_accuracy)\n",
        "\n",
        "                # Learning rate scheduling\n",
        "                self.scheduler.step(val_loss)\n",
        "\n",
        "                # Early stopping tracking\n",
        "                improved = val_loss < best_val_loss\n",
        "                if improved:\n",
        "                    best_val_loss = val_loss\n",
        "                    patience_counter = 0\n",
        "                    # Save a deep copy of the best state dict\n",
        "                    best_state_dict = copy.deepcopy(self.model.state_dict())\n",
        "                else:\n",
        "                    patience_counter += 1\n",
        "\n",
        "                if patience_counter >= patience:\n",
        "                    print(f\"Early stopping at epoch {epoch}\")\n",
        "                    break\n",
        "\n",
        "                # Print statement with accuracy\n",
        "                print(f\"Epoch {epoch}: Train Loss: {train_loss/len(train_loader):.4f}, Train Acc: {train_accuracy:.4f} | \"\n",
        "                      f\"Val Loss: {val_loss:.4f}, Val AUC: {val_auc:.4f}, Val Acc: {val_accuracy:.4f}\")\n",
        "\n",
        "\n",
        "            history['train_loss'].append(train_loss/len(train_loader))\n",
        "\n",
        "        # Restore best model weights if available\n",
        "        if 'best_state_dict' in locals() and best_state_dict is not None:\n",
        "            self.model.load_state_dict(best_state_dict)\n",
        "            self.best_val_loss = best_val_loss\n",
        "            self.best_state_dict = best_state_dict\n",
        "        else:\n",
        "            print(\"No validation data provided - using final model weights\")\n",
        "            self.best_val_loss = None\n",
        "            self.best_state_dict = None\n",
        "\n",
        "        return history\n",
        "\n",
        "    def _validate(self, val_loader):\n",
        "        self.model.eval()\n",
        "        val_loss = 0\n",
        "        all_probs = []\n",
        "        all_labels = []\n",
        "\n",
        "        # Track validation accuracy\n",
        "        val_correct = 0\n",
        "        val_total = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch_num, batch_y in val_loader:\n",
        "                # Move tensors to device with non_blocking for CUDA efficiency\n",
        "                batch_num = batch_num.to(self.device, non_blocking=True)\n",
        "                batch_y = batch_y.to(self.device, non_blocking=True)\n",
        "\n",
        "                outputs = self.model(batch_num, None)\n",
        "                loss = self.criterion(outputs, batch_y.squeeze())\n",
        "                val_loss += loss.item()\n",
        "\n",
        "                probs = torch.softmax(outputs, dim=1)[:, 1]\n",
        "                all_probs.extend(probs.cpu().numpy())\n",
        "                all_labels.extend(batch_y.cpu().numpy())\n",
        "\n",
        "                # Calculate validation accuracy\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                val_total += batch_y.size(0)\n",
        "                val_correct += (predicted == batch_y.squeeze()).sum().item()\n",
        "\n",
        "        val_accuracy = val_correct / val_total\n",
        "        val_auc = roc_auc_score(all_labels, all_probs)\n",
        "        return val_loss/len(val_loader), val_auc, val_accuracy\n",
        "\n",
        "    def predict_proba(self, X_num, batch_size=512):\n",
        "        self.model.eval()\n",
        "        all_probs = []\n",
        "\n",
        "        # CUDA optimizations for inference\n",
        "        pin_memory = torch.cuda.is_available() and self.device.type == 'cuda'\n",
        "\n",
        "        dataset = TensorDataset(\n",
        "            torch.FloatTensor(X_num)\n",
        "        )\n",
        "        loader = DataLoader(\n",
        "            dataset,\n",
        "            batch_size=batch_size,\n",
        "            pin_memory=pin_memory,\n",
        "            num_workers=0\n",
        "        )\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch in loader:\n",
        "                batch_num = batch[0].to(self.device, non_blocking=True)\n",
        "\n",
        "                outputs = self.model(batch_num, None)\n",
        "                probs = torch.softmax(outputs, dim=1)\n",
        "                all_probs.extend(probs.cpu().numpy())\n",
        "\n",
        "        return np.array(all_probs)\n",
        "\n",
        "    def get_memory_usage(self):\n",
        "        \"\"\"Get current memory usage information\"\"\"\n",
        "        if torch.cuda.is_available() and self.device.type == 'cuda':\n",
        "            allocated = torch.cuda.memory_allocated(self.device) / 1024**2\n",
        "            cached = torch.cuda.memory_reserved(self.device) / 1024**2\n",
        "            return f\"GPU Memory - Allocated: {allocated:.1f} MB, Cached: {cached:.1f} MB\"\n",
        "        else:\n",
        "            return \"CPU mode - no GPU memory tracking\"\n",
        "\n",
        "    def cleanup_memory(self):\n",
        "        \"\"\"Clean up CUDA memory and force garbage collection\"\"\"\n",
        "        import gc\n",
        "\n",
        "        # Force garbage collection to clean up any lingering DataLoader references\n",
        "        gc.collect()\n",
        "\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.empty_cache()\n",
        "            print(\"CUDA cache cleared\")\n",
        "\n",
        "        print(\"Memory cleanup completed\")\n",
        "\n",
        "    def save_model(self, filepath):\n",
        "        \"\"\"Save the best model state and configuration\"\"\"\n",
        "        save_dict = {\n",
        "            'model_state_dict': self.best_state_dict if hasattr(self, 'best_state_dict') and self.best_state_dict is not None else self.model.state_dict(),\n",
        "            'model_params': self.model_params,\n",
        "            'best_val_loss': getattr(self, 'best_val_loss', None),\n",
        "            'device': str(self.device),\n",
        "            'seed': self.seed\n",
        "        }\n",
        "        torch.save(save_dict, filepath)\n",
        "        print(f\"Model saved to {filepath}\")\n",
        "\n",
        "    def load_model(self, filepath):\n",
        "        \"\"\"Load model state and configuration\"\"\"\n",
        "        checkpoint = torch.load(filepath, map_location=self.device)\n",
        "        self.model.load_state_dict(checkpoint['model_state_dict'])\n",
        "        self.best_val_loss = checkpoint.get('best_val_loss', None)\n",
        "        self.best_state_dict = checkpoint['model_state_dict']\n",
        "        print(f\"Model loaded from {filepath}\")\n",
        "        if self.best_val_loss is not None:\n",
        "            print(f\"Best validation loss: {self.best_val_loss:.4f}\")\n",
        "\n",
        "    def ensure_best_model(self):\n",
        "        \"\"\"Ensure the model is using the best weights\"\"\"\n",
        "        if hasattr(self, 'best_state_dict') and self.best_state_dict is not None:\n",
        "            self.model.load_state_dict(self.best_state_dict)\n",
        "            print(\"Using best model weights for evaluation\")\n",
        "        else:\n",
        "            print(\"No best model weights available, using current weights\")\n",
        "\n",
        "X_train_num = X_train_split[numerical_cols].reset_index(drop=True).to_numpy()\n",
        "y_train = y_train_split.reset_index(drop=True).to_numpy()\n",
        "\n",
        "X_val_num = X_val[numerical_cols].reset_index(drop=True).to_numpy()\n",
        "y_val_val = y_val.reset_index(drop=True).to_numpy()\n",
        "\n",
        "categorical_cols = []\n",
        "cat_dims = []"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TpTgc8ej_3PC"
      },
      "source": [
        "## 4. Hyperparameter Grid Search\n",
        "\n",
        "Performing grid search to find the optimal hyperparameters for the FT-Transformer model.\n",
        "\n",
        "### Grid Search Implementation\n",
        "\n",
        "We'll search over key hyperparameters that significantly impact model performance:\n",
        "- Model architecture parameters (d_block, n_blocks, attention_n_heads)\n",
        "- Regularization parameters (attention_dropout, ffn_dropout)\n",
        "- Training parameters (learning rate, batch size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 689
        },
        "id": "G7EWLd6e_3PC",
        "outputId": "75c0ab45-1338-4506-f57f-57f77f2a2af2"
      },
      "outputs": [],
      "source": [
        "import itertools\n",
        "from sklearn.metrics import roc_auc_score, average_precision_score\n",
        "import os\n",
        "import json\n",
        "from datetime import datetime\n",
        "\n",
        "class GridSearchFTTransformer:\n",
        "    \"\"\"\n",
        "    Grid search implementation for FT-Transformer hyperparameter optimization\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, X_train, y_train, X_val, y_val, X_test, y_test, feature_names, numerical_cols,\n",
        "                 categorical_cols=None, cat_dims=None, device=None, seed=42):\n",
        "        self.X_train = X_train\n",
        "        self.y_train = y_train\n",
        "        self.X_val = X_val\n",
        "        self.y_val = y_val\n",
        "        self.X_test = X_test\n",
        "        self.y_test = y_test\n",
        "        self.feature_names = feature_names\n",
        "        self.numerical_cols = numerical_cols\n",
        "        self.categorical_cols = categorical_cols or []\n",
        "        self.cat_dims = cat_dims or []\n",
        "        self.device = device or globals().get('device', torch.device('cuda' if torch.cuda.is_available() else 'cpu'))\n",
        "        self.seed = seed\n",
        "\n",
        "        self.results = []\n",
        "        self.best_params = None\n",
        "        self.best_score = -np.inf\n",
        "        self.best_model = None\n",
        "\n",
        "    def define_search_space(self, search_type='quick'):\n",
        "        \"\"\"\n",
        "        Define hyperparameter search space\n",
        "\n",
        "        Args:\n",
        "            search_type: 'quick' for fast search, 'comprehensive' for thorough search\n",
        "        \"\"\"\n",
        "        if search_type == 'quick':\n",
        "            # Quick search with fewer combinations\n",
        "            param_grid = {\n",
        "                # Model architecture\n",
        "                'd_block': [128, 192, 256],\n",
        "                'n_blocks': [2, 3],\n",
        "                'attention_n_heads': [4, 8],\n",
        "\n",
        "                # Regularization\n",
        "                'attention_dropout': [0.1, 0.2],\n",
        "                'ffn_dropout': [0.1, 0.2],\n",
        "\n",
        "                # Training parameters\n",
        "                'learning_rate': [0.0003, 0.0005, 0.001],\n",
        "                'batch_size_multiplier': [1, 2]\n",
        "            }\n",
        "        else:  # comprehensive\n",
        "            param_grid = {\n",
        "                # Model architecture\n",
        "                'd_block': [96, 128, 192, 256, 320],\n",
        "                'n_blocks': [1, 2, 3, 4],\n",
        "                'attention_n_heads': [2, 4, 6, 8, 10, 12],\n",
        "\n",
        "                # Regularization\n",
        "                'attention_dropout': [0.0, 0.1, 0.2, 0.3],\n",
        "                'ffn_dropout': [0.0, 0.1, 0.2, 0.3],\n",
        "                'residual_dropout': [0.0, 0.1],\n",
        "\n",
        "                # Training parameters\n",
        "                'learning_rate': [0.0001, 0.0003, 0.0005, 0.001, 0.002],\n",
        "                'batch_size_multiplier': [0.5, 1, 2, 4]\n",
        "            }\n",
        "\n",
        "        return param_grid\n",
        "\n",
        "    def create_param_combinations(self, param_grid, max_combinations=None):\n",
        "        \"\"\"Create all parameter combinations\"\"\"\n",
        "        keys = list(param_grid.keys())\n",
        "        values = list(param_grid.values())\n",
        "\n",
        "        combinations = list(itertools.product(*values))\n",
        "\n",
        "        if max_combinations and len(combinations) > max_combinations:\n",
        "            # Randomly sample combinations if too many\n",
        "            np.random.seed(self.seed)\n",
        "            indices = np.random.choice(len(combinations), max_combinations, replace=False)\n",
        "            combinations = [combinations[i] for i in indices]\n",
        "            print(f\"Randomly sampled {max_combinations} combinations from {len(list(itertools.product(*values)))}\")\n",
        "\n",
        "        param_combinations = []\n",
        "        for combo in combinations:\n",
        "            param_dict = dict(zip(keys, combo))\n",
        "            param_combinations.append(param_dict)\n",
        "\n",
        "        return param_combinations\n",
        "\n",
        "    def train_single_model(self, params, trial_num, total_trials):\n",
        "        \"\"\"Train a single model with given parameters\"\"\"\n",
        "        print(f\"\\nTrial {trial_num}/{total_trials}\")\n",
        "        print(f\"Parameters: {params}\")\n",
        "\n",
        "        try:\n",
        "            # Extract training parameters\n",
        "            learning_rate = params.pop('learning_rate', 0.0005)\n",
        "            batch_size_multiplier = params.pop('batch_size_multiplier', 1)\n",
        "\n",
        "                            # Calculate batch size\n",
        "            base_batch_size = 4096 if torch.cuda.is_available() and self.device.type == 'cuda' else 1024\n",
        "            batch_size = int(base_batch_size * batch_size_multiplier)\n",
        "\n",
        "            print(f\"🔧 Using batch_size: {batch_size} (base: {base_batch_size}, multiplier: {batch_size_multiplier})\")\n",
        "\n",
        "            # Create model with current parameters\n",
        "            model = FraudFTTransformer(\n",
        "                n_num_features=len(self.feature_names),\n",
        "                n_cat_features=len(self.categorical_cols),\n",
        "                n_cont_features=len(self.numerical_cols),\n",
        "                cat_cardinalities=self.cat_dims,\n",
        "                device=self.device,\n",
        "                seed=self.seed,\n",
        "                **params  # Pass model architecture parameters\n",
        "            )\n",
        "\n",
        "            # Update learning rate\n",
        "            for param_group in model.optimizer.param_groups:\n",
        "                param_group['lr'] = learning_rate\n",
        "\n",
        "            # Train model with early stopping\n",
        "            history = model.fit(\n",
        "                self.X_train, self.y_train,\n",
        "                self.X_val, self.y_val,\n",
        "                epochs=17,  # Reduced epochs for grid search\n",
        "                batch_size=batch_size,\n",
        "                patience=5\n",
        "            )\n",
        "\n",
        "            # Ensure we're using the best model for evaluation\n",
        "            model.ensure_best_model()\n",
        "\n",
        "            # Evaluate on TEST set using both ROC-AUC and PR-AUC\n",
        "            test_probs = model.predict_proba(self.X_test)[:, 1]\n",
        "            test_roc_auc = roc_auc_score(self.y_test, test_probs)\n",
        "            test_pr_auc = average_precision_score(self.y_test, test_probs)\n",
        "\n",
        "            # Get best validation loss from training\n",
        "            best_val_loss = getattr(model, 'best_val_loss', None)\n",
        "            if best_val_loss is None and history['val_loss']:\n",
        "                best_val_loss = min(history['val_loss'])\n",
        "\n",
        "            result = {\n",
        "                'trial': trial_num,\n",
        "                'params': {**params, 'learning_rate': learning_rate, 'batch_size': batch_size},\n",
        "                'test_roc_auc': float(test_roc_auc),\n",
        "                'test_pr_auc': float(test_pr_auc),  # Primary metric for model selection\n",
        "                'val_loss': float(best_val_loss),\n",
        "                'training_epochs': len(history['val_auc']) if history['val_auc'] else 0,\n",
        "                'timestamp': datetime.now().isoformat()\n",
        "            }\n",
        "\n",
        "            print(f\"Trial {trial_num} completed - Test PR-AUC: {test_pr_auc:.4f}, Test ROC-AUC: {test_roc_auc:.4f}, Val Loss: {best_val_loss:.4f}\")\n",
        "\n",
        "            # Update best model if this is better (using PR-AUC as primary metric)\n",
        "            if test_pr_auc > self.best_score:\n",
        "                self.best_score = test_pr_auc\n",
        "                self.best_params = result['params'].copy()\n",
        "                # Save best model\n",
        "                # model.save_model(f'../models/ft_transformer_best_trial_{trial_num}.pth')\n",
        "                self.best_model = model\n",
        "                print(f\"New best model! Test PR-AUC: {test_pr_auc:.4f}, Test ROC-AUC: {test_roc_auc:.4f}\")\n",
        "\n",
        "                            # Cleanup\n",
        "                model.cleanup_memory()\n",
        "                del model\n",
        "\n",
        "                # Additional cleanup for Jupyter\n",
        "                import gc\n",
        "                gc.collect()\n",
        "\n",
        "            return result\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Trial {trial_num} failed: {str(e)}\")\n",
        "            return {\n",
        "                'trial': trial_num,\n",
        "                'params': params,\n",
        "                'test_roc_auc': -1,\n",
        "                'test_pr_auc': -1,\n",
        "                'val_loss': float('inf'),\n",
        "                'error': str(e),\n",
        "                'timestamp': datetime.now().isoformat()\n",
        "            }\n",
        "\n",
        "    def run_grid_search(self, search_type='quick', max_combinations=20, save_results=True):\n",
        "        \"\"\"\n",
        "        Run the complete grid search\n",
        "\n",
        "        Args:\n",
        "            search_type: 'quick' or 'comprehensive'\n",
        "            max_combinations: Maximum number of combinations to try\n",
        "            save_results: Whether to save results to file\n",
        "        \"\"\"\n",
        "        print(f\"Starting {search_type} grid search for FT-Transformer\")\n",
        "        print(f\"Maximum combinations: {max_combinations}\")\n",
        "        print(f\"Device: {self.device}\")\n",
        "\n",
        "        # Define search space\n",
        "        param_grid = self.define_search_space(search_type)\n",
        "        print(f\"Search space: {param_grid}\")\n",
        "\n",
        "        # Create parameter combinations\n",
        "        param_combinations = self.create_param_combinations(param_grid, max_combinations)\n",
        "        total_trials = len(param_combinations)\n",
        "\n",
        "        print(f\"Total trials to run: {total_trials}\")\n",
        "\n",
        "        # Run trials\n",
        "        start_time = datetime.now()\n",
        "\n",
        "        for i, params in enumerate(param_combinations, 1):\n",
        "            result = self.train_single_model(params.copy(), i, total_trials)\n",
        "            self.results.append(result)\n",
        "\n",
        "            # Save intermediate results\n",
        "            if save_results and i % 5 == 0:\n",
        "                self.save_results(f'../results/grid_search_intermediate_{i}.json')\n",
        "\n",
        "        end_time = datetime.now()\n",
        "        duration = end_time - start_time\n",
        "\n",
        "        print(f\"\\nGrid search completed!\")\n",
        "        print(f\"Total time: {duration}\")\n",
        "        print(f\"Best PR-AUC: {self.best_score:.4f}\")\n",
        "        print(f\"Best parameters: {self.best_params}\")\n",
        "\n",
        "        if save_results:\n",
        "            self.save_results('../results/grid_search_final_results.json')\n",
        "\n",
        "        print(self.results)\n",
        "        return self.results\n",
        "\n",
        "    def save_results(self, filepath):\n",
        "        \"\"\"Save grid search results\"\"\"\n",
        "        os.makedirs(os.path.dirname(filepath), exist_ok=True)\n",
        "\n",
        "        results_summary = {\n",
        "            'search_completed': datetime.now().isoformat(),\n",
        "            'total_trials': len(self.results),\n",
        "            'best_score': self.best_score,\n",
        "            'best_params': self.best_params,\n",
        "            'device_used': str(self.device),\n",
        "            'all_results': self.results\n",
        "        }\n",
        "\n",
        "        with open(filepath, 'w') as f:\n",
        "            json.dump(results_summary, f, indent=2, default=str)\n",
        "\n",
        "        print(f\"Results saved to {filepath}\")\n",
        "\n",
        "    def get_top_results(self, n=5):\n",
        "        \"\"\"Get top N results based on PR-AUC (TEST set)\"\"\"\n",
        "        if not self.results:\n",
        "            return []\n",
        "\n",
        "        # Filter out failed trials and sort by PR-AUC (TEST set)\n",
        "        valid_results = [r for r in self.results if r['test_pr_auc'] > 0]\n",
        "        sorted_results = sorted(valid_results, key=lambda x: x['test_pr_auc'], reverse=True)\n",
        "\n",
        "        return sorted_results[:n]\n",
        "\n",
        "    def plot_results(self):\n",
        "        \"\"\"Plot grid search results\"\"\"\n",
        "        if not self.results:\n",
        "            print(\"No results to plot\")\n",
        "            return\n",
        "\n",
        "        valid_results = [r for r in self.results if r['test_pr_auc'] > 0]\n",
        "\n",
        "        if not valid_results:\n",
        "            print(\"No valid results to plot\")\n",
        "            return\n",
        "\n",
        "        # Extract data for plotting (TEST set metrics)\n",
        "        pr_aucs = [r['test_pr_auc'] for r in valid_results]\n",
        "        roc_aucs = [r['test_roc_auc'] for r in valid_results]\n",
        "        trials = [r['trial'] for r in valid_results]\n",
        "\n",
        "        plt.figure(figsize=(15, 10))\n",
        "\n",
        "        # Plot 1: PR-AUC by trial\n",
        "        plt.subplot(2, 3, 1)\n",
        "        plt.plot(trials, pr_aucs, 'bo-', alpha=0.7)\n",
        "        plt.axhline(y=self.best_score, color='r', linestyle='--', label=f'Best PR-AUC: {self.best_score:.4f}')\n",
        "        plt.xlabel('Trial')\n",
        "        plt.ylabel('Validation PR-AUC')\n",
        "        plt.title('Validation PR-AUC by Trial')\n",
        "        plt.legend()\n",
        "        plt.grid(True, alpha=0.3)\n",
        "\n",
        "        # Plot 2: ROC-AUC by trial\n",
        "        plt.subplot(2, 3, 2)\n",
        "        plt.plot(trials, roc_aucs, 'go-', alpha=0.7)\n",
        "        plt.xlabel('Trial')\n",
        "        plt.ylabel('Validation ROC-AUC')\n",
        "        plt.title('Validation ROC-AUC by Trial')\n",
        "        plt.grid(True, alpha=0.3)\n",
        "\n",
        "        # Plot 3: PR-AUC distribution\n",
        "        plt.subplot(2, 3, 3)\n",
        "        plt.hist(pr_aucs, bins=min(20, len(pr_aucs)//2), alpha=0.7, edgecolor='black')\n",
        "        plt.axvline(x=self.best_score, color='r', linestyle='--', label=f'Best PR-AUC: {self.best_score:.4f}')\n",
        "        plt.xlabel('Validation PR-AUC')\n",
        "        plt.ylabel('Frequency')\n",
        "        plt.title('Distribution of Validation PR-AUC')\n",
        "        plt.legend()\n",
        "        plt.grid(True, alpha=0.3)\n",
        "\n",
        "        # Plot 4: Parameter analysis (if we have enough data)\n",
        "        if len(valid_results) > 5:\n",
        "            plt.subplot(2, 3, 4)\n",
        "            # Analyze d_block parameter\n",
        "            d_blocks = [r['params'].get('d_block', 192) for r in valid_results]\n",
        "            pr_auc_by_d_block = {}\n",
        "            for db, pr_auc in zip(d_blocks, pr_aucs):\n",
        "                if db not in pr_auc_by_d_block:\n",
        "                    pr_auc_by_d_block[db] = []\n",
        "                pr_auc_by_d_block[db].append(pr_auc)\n",
        "\n",
        "            db_means = {k: np.mean(v) for k, v in pr_auc_by_d_block.items()}\n",
        "            plt.bar(db_means.keys(), db_means.values(), alpha=0.7)\n",
        "            plt.xlabel('d_block')\n",
        "            plt.ylabel('Mean Validation PR-AUC')\n",
        "            plt.title('Performance by d_block')\n",
        "            plt.grid(True, alpha=0.3)\n",
        "\n",
        "        # Plot 5: PR-AUC vs ROC-AUC scatter\n",
        "        plt.subplot(2, 3, 5)\n",
        "        plt.scatter(roc_aucs, pr_aucs, alpha=0.7)\n",
        "        plt.xlabel('ROC-AUC')\n",
        "        plt.ylabel('PR-AUC')\n",
        "        plt.title('PR-AUC vs ROC-AUC')\n",
        "        plt.grid(True, alpha=0.3)\n",
        "\n",
        "        # Plot 6: Top results\n",
        "        plt.subplot(2, 3, 6)\n",
        "        top_results = self.get_top_results(min(10, len(valid_results)))\n",
        "        top_pr_aucs = [r['test_pr_auc'] for r in top_results]\n",
        "        top_trials = [r['trial'] for r in top_results]\n",
        "\n",
        "        plt.barh(range(len(top_pr_aucs)), top_pr_aucs, alpha=0.7)\n",
        "        plt.yticks(range(len(top_pr_aucs)), [f\"Trial {t}\" for t in top_trials])\n",
        "        plt.xlabel('Test PR-AUC')\n",
        "        plt.title(f'Top {len(top_results)} Results (Test PR-AUC)')\n",
        "        plt.grid(True, alpha=0.3)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "def create_safe_dataloader(dataset, batch_size, shuffle=False, pin_memory=False):\n",
        "    \"\"\"\n",
        "    Create a Jupyter-safe DataLoader with proper configuration\n",
        "    \"\"\"\n",
        "    return DataLoader(\n",
        "        dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=shuffle,\n",
        "        pin_memory=pin_memory,\n",
        "        num_workers=0,  # Always 0 for Jupyter compatibility\n",
        "        drop_last=False\n",
        "    )\n",
        "\n",
        "print(\"Grid search implementation ready!\")\n",
        "print(\"DataLoader configuration optimized for Jupyter notebook environment\")\n",
        "\n",
        "# Create directories for saving results and models\n",
        "# os.makedirs('../results', exist_ok=True)\n",
        "# os.makedirs('../models', exist_ok=True)\n",
        "\n",
        "# Initialize grid search\n",
        "# Prepare test data for grid search\n",
        "X_test_df = pd.DataFrame(X_test, columns=feature_names)\n",
        "X_test_num_gs = X_test_df[numerical_cols].reset_index(drop=True).to_numpy()\n",
        "\n",
        "grid_search = GridSearchFTTransformer(\n",
        "    X_train=X_train_num,\n",
        "    y_train=y_train,\n",
        "    X_val=X_val_num,\n",
        "    y_val=y_val_val,\n",
        "    X_test=X_test_num_gs,\n",
        "    y_test=y_test,\n",
        "    feature_names=feature_names,\n",
        "    numerical_cols=numerical_cols,\n",
        "    categorical_cols=categorical_cols,\n",
        "    cat_dims=cat_dims,\n",
        "    device=device,\n",
        "    seed=random_seed\n",
        ")\n",
        "\n",
        "# Run grid search (quick version for demonstration)\n",
        "# You can change to 'comprehensive' for more thorough search\n",
        "search_results = grid_search.run_grid_search(\n",
        "    search_type='comp',  # or 'comprehensive'\n",
        "    max_combinations=100,  # Adjust based on your time/compute budget\n",
        "    save_results=False\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xhWGm-D7_3PD",
        "outputId": "bfb0c631-8a67-47c1-a831-a9501233ca89"
      },
      "outputs": [],
      "source": [
        "# Analyze grid search results\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"GRID SEARCH RESULTS ANALYSIS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Display top results\n",
        "top_results = grid_search.get_top_results(5)\n",
        "print(f\"\\nTop 5 Results (ranked by Test PR-AUC):\")\n",
        "for i, result in enumerate(top_results, 1):\n",
        "    print(f\"\\n{i}. Trial {result['trial']} - Test PR-AUC: {result['test_pr_auc']:.4f}, Test ROC-AUC: {result['test_roc_auc']:.4f}\")\n",
        "    print(f\"   Parameters: {result['params']}\")\n",
        "\n",
        "# Plot results\n",
        "print(f\"\\nPlotting grid search results...\")\n",
        "grid_search.plot_results()\n",
        "\n",
        "# Get the best model for final evaluation\n",
        "best_model = grid_search.best_model\n",
        "if best_model is not None:\n",
        "    print(f\"\\nBest model loaded with Test PR-AUC: {grid_search.best_score:.4f}\")\n",
        "    print(f\"Best parameters: {grid_search.best_params}\")\n",
        "\n",
        "    # Ensure we're using the best weights\n",
        "    best_model.ensure_best_model()\n",
        "\n",
        "    # Save the best model as the final model\n",
        "    # best_model.save_model('../models/ft_transformer_best_final.pth')\n",
        "\n",
        "    # Update the global ft_model to use the best one\n",
        "    ft_model = best_model\n",
        "    print(\"Updated global ft_model to use the best hyperparameters\")\n",
        "else:\n",
        "    print(\"No best model found, using original model\")\n",
        "\n",
        "print(f\"\\nGrid search completed successfully!\")\n",
        "print(f\"Final model memory usage: {ft_model.get_memory_usage()}\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "01aa62cc59114a2d878177141b4efa93": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "14c5016570ed438fbefd0784e511f779": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_01aa62cc59114a2d878177141b4efa93",
            "placeholder": "​",
            "style": "IPY_MODEL_92940135e5a7401c911187eff1706108",
            "value": "100%"
          }
        },
        "1a68880b702044bc84f1fc598dd3e070": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_14c5016570ed438fbefd0784e511f779",
              "IPY_MODEL_d73ff4171f554cca8dbaab1ac8080b75",
              "IPY_MODEL_cb6e27dbf3eb40ef9c8c881b21e32cf3"
            ],
            "layout": "IPY_MODEL_51accfdf01b8403d95f120c461387792"
          }
        },
        "1d121c3454a04220bbfa06b744b7d4e0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2854d5d211e948d3a8123cefe130be64": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "51accfdf01b8403d95f120c461387792": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "73ec11fb91014226a08a4b0cc0e2d5d3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "92940135e5a7401c911187eff1706108": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cb6e27dbf3eb40ef9c8c881b21e32cf3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_73ec11fb91014226a08a4b0cc0e2d5d3",
            "placeholder": "​",
            "style": "IPY_MODEL_2854d5d211e948d3a8123cefe130be64",
            "value": " 500/500 [42:12&lt;00:00,  5.07s/it]"
          }
        },
        "d73ff4171f554cca8dbaab1ac8080b75": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1d121c3454a04220bbfa06b744b7d4e0",
            "max": 500,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e231f532dac94bd692ef063357ffd7fe",
            "value": 500
          }
        },
        "e231f532dac94bd692ef063357ffd7fe": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
