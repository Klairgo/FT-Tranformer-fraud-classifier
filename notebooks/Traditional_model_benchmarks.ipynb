{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Dataset2 Model Benchmarks\n",
        "\n",
        "This notebook evaluates multiple classical and neural classifiers on Dataset2 using the preprocessed train/test CSVs.\n",
        "\n",
        "Models included:\n",
        "- Logistic Regression (balanced)\n",
        "- Decision Tree (balanced)\n",
        "- Random Forest (balanced)\n",
        "- Gradient Boosting\n",
        "- Linear SVM (balanced)\n",
        "- MLPClassifier (neural network)\n",
        "- Optional: XGBoost (if installed)\n",
        "\n",
        "For each model, we compute:\n",
        "- Accuracy, Precision, Recall, F1\n",
        "- ROC-AUC, PR-AUC\n",
        "- Confusion Matrix\n",
        "- Classification Report\n",
        "\n",
        "All results are summarized for easy comparison with your FT-Transformer.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Paths and configuration\n",
        "train_dataset_path = '../data/train.csv'\n",
        "test_dataset_path = '../data/test.csv'\n",
        "metadata_path = '../data/preprocessing_metadata.json'\n",
        "class_label = 'Class'\n",
        "random_seed = 30\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Imports\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score, f1_score,\n",
        "    confusion_matrix, classification_report, roc_auc_score,\n",
        "    precision_recall_curve, roc_curve, average_precision_score\n",
        ")\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "try:\n",
        "    from xgboost import XGBClassifier\n",
        "    XGB_AVAILABLE = True\n",
        "except Exception:\n",
        "    XGB_AVAILABLE = False\n",
        "\n",
        "# For type hints\n",
        "from typing import Dict\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading Dataset2...\n",
            "(409412, 31) (28481, 31)\n",
            "Train: (409412, 30), Test: (28481, 30)\n",
            "Class balance (train): [255883 153529]\n",
            "Class balance (test): [28432    49]\n"
          ]
        }
      ],
      "source": [
        "print('Loading Dataset...')\n",
        "train_df = pd.read_csv(train_dataset_path)\n",
        "test_df = pd.read_csv(test_dataset_path)\n",
        "print(train_df.shape, test_df.shape)\n",
        "\n",
        "# Identify features/target\n",
        "feature_cols = [c for c in train_df.columns if c != class_label]\n",
        "X_train = train_df[feature_cols].copy()\n",
        "y_train = train_df[class_label].astype(int).values\n",
        "X_test = test_df[feature_cols].copy()\n",
        "y_test = test_df[class_label].astype(int).values\n",
        "\n",
        "print(f\"Train: {X_train.shape}, Test: {X_test.shape}\")\n",
        "print('Class balance (train):', np.bincount(y_train))\n",
        "print('Class balance (test):', np.bincount(y_test))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Utility: evaluate model\n",
        "from typing import Dict\n",
        "\n",
        "\n",
        "def format_classification_report_latex(y_true, y_pred, class_names=(\"Non-Fraud\", \"Fraud\"), digits: int = 4) -> str:\n",
        "    \"\"\"Return a LaTeX table for the classification report matching the requested template.\"\"\"\n",
        "    rep = classification_report(\n",
        "        y_true,\n",
        "        y_pred,\n",
        "        target_names=list(class_names),\n",
        "        output_dict=True,\n",
        "        zero_division=0,\n",
        "        digits=digits,\n",
        "    )\n",
        "\n",
        "    def fmt(x: float) -> str:\n",
        "        return f\"{x:.{digits}f}\"\n",
        "\n",
        "    non_fraud = rep[class_names[0]]\n",
        "    fraud = rep[class_names[1]]\n",
        "    accuracy_val = rep[\"accuracy\"]\n",
        "    macro_avg = rep[\"macro avg\"]\n",
        "    weighted_avg = rep[\"weighted avg\"]\n",
        "    total_support = int(non_fraud[\"support\"] + fraud[\"support\"])\n",
        "\n",
        "    lines = []\n",
        "    lines.append(r\"\\begin{table}[h!]\")\n",
        "    lines.append(r\"\\centering\")\n",
        "    lines.append(r\"\\begin{tabular}{lcccc}\")\n",
        "    lines.append(r\"\\hline\")\n",
        "    lines.append(r\"\\textbf{Class} & \\textbf{Precision} & \\textbf{Recall} & \\textbf{F1-Score} & \\textbf{Support} \\\\\")\n",
        "    lines.append(r\"\\hline\")\n",
        "    lines.append(f\"Non-Fraud      & {fmt(non_fraud['precision'])} & {fmt(non_fraud['recall'])} & {fmt(non_fraud['f1-score'])} & {int(non_fraud['support'])} \\\\\\\\\")\n",
        "    lines.append(f\"Fraud          & {fmt(fraud['precision'])} & {fmt(fraud['recall'])} & {fmt(fraud['f1-score'])} & {int(fraud['support'])} \\\\\\\\\")\n",
        "    lines.append(r\"\\hline\")\n",
        "    lines.append(f\"Accuracy       &        &        & {fmt(accuracy_val)} & {total_support} \\\\\\\\\")\n",
        "    lines.append(f\"Macro Avg      & {fmt(macro_avg['precision'])} & {fmt(macro_avg['recall'])} & {fmt(macro_avg['f1-score'])} & {total_support} \\\\\\\\\")\n",
        "    lines.append(f\"Weighted Avg   & {fmt(weighted_avg['precision'])} & {fmt(weighted_avg['recall'])} & {fmt(weighted_avg['f1-score'])} & {total_support} \\\\\\\\\")\n",
        "    lines.append(r\"\\hline\")\n",
        "    lines.append(r\"\\end{tabular}\")\n",
        "    lines.append(r\"\\caption{Classification Report}\")\n",
        "    lines.append(r\"\\label{tab:classification_report}\")\n",
        "    lines.append(r\"\\end{table}\")\n",
        "\n",
        "    return \"\\n\".join(lines)\n",
        "\n",
        "\n",
        "def evaluate_classifier(model, X_test, y_test, name: str) -> Dict:\n",
        "    y_pred = model.predict(X_test)\n",
        "    if hasattr(model, 'predict_proba'):\n",
        "        y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
        "    elif hasattr(model, 'decision_function'):\n",
        "        # For LinearSVC\n",
        "        scores = model.decision_function(X_test)\n",
        "        # Convert scores to [0,1] via min-max for AUC/PR (ranking is what matters)\n",
        "        scores = (scores - scores.min()) / (scores.max() - scores.min() + 1e-8)\n",
        "        y_pred_proba = scores\n",
        "    else:\n",
        "        y_pred_proba = (y_pred == 1).astype(float)\n",
        "    \n",
        "    metrics = {\n",
        "        'model': name,\n",
        "        'accuracy': accuracy_score(y_test, y_pred),\n",
        "        'precision': precision_score(y_test, y_pred, zero_division=0),\n",
        "        'recall': recall_score(y_test, y_pred, zero_division=0),\n",
        "        'f1': f1_score(y_test, y_pred, zero_division=0),\n",
        "        'roc_auc': roc_auc_score(y_test, y_pred_proba),\n",
        "        'pr_auc': precision_recall_curve(y_test, y_pred_proba),\n",
        "        'confusion_matrix': confusion_matrix(y_test, y_pred)\n",
        "    }\n",
        "    \n",
        "    print(f\"\\n=== {name} ===\")\n",
        "    print(f\"Accuracy: {metrics['accuracy']:.4f} | Precision: {metrics['precision']:.4f} | Recall: {metrics['recall']:.4f} | F1: {metrics['f1']:.4f}\")\n",
        "    print(f\"ROC-AUC: {metrics['roc_auc']:.4f} | PR-AUC: {metrics['pr_auc']:.4f}\")\n",
        "    print(\"Confusion Matrix:\\n\", metrics['confusion_matrix'])\n",
        "\n",
        "    # Print LaTeX-formatted classification report and save alongside results for easy copy/paste\n",
        "    latex_report = format_classification_report_latex(y_test, y_pred, class_names=(\"Non-Fraud\", \"Fraud\"), digits=4)\n",
        "    print(\"\\nLaTeX Classification Report:\\n\")\n",
        "    print(latex_report)\n",
        "\n",
        "    # Also save to the results directory\n",
        "    # out_dir = Path('../results')\n",
        "    # out_dir.mkdir(parents=True, exist_ok=True)\n",
        "    # with open(out_dir / f\"dataset2_{name.lower()}_classification_report.tex\", 'w') as f:\n",
        "    #     f.write(latex_report + \"\\n\")\n",
        "    \n",
        "    return metrics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defined 6 models\n"
          ]
        }
      ],
      "source": [
        "# Define models\n",
        "models = []\n",
        "\n",
        "# Logistic Regression (balanced)\n",
        "models.append(('LogisticRegression', LogisticRegression(max_iter=100000, class_weight='balanced', n_jobs=None)))\n",
        "\n",
        "# # Decision Tree (balanced)\n",
        "models.append(('DecisionTree', DecisionTreeClassifier(class_weight='balanced', random_state=random_seed)))\n",
        "\n",
        "# Random Forest (balanced)\n",
        "models.append(('RandomForest', RandomForestClassifier(n_estimators=100, max_depth=None, n_jobs=-1, class_weight='balanced_subsample', random_state=random_seed)))\n",
        "\n",
        "# # Gradient Boosting\n",
        "models.append(('GradientBoosting', GradientBoostingClassifier(random_state=random_seed)))\n",
        "\n",
        "# # Linear SVM (use LinearSVC, which does not output proba)\n",
        "models.append(('LinearSVM', LinearSVC(class_weight='balanced', random_state=random_seed)))\n",
        "\n",
        "# MLP (neural network)\n",
        "models.append(('MLP', MLPClassifier(hidden_layer_sizes=(128, 64), activation='relu', solver='adam', alpha=1e-4,\n",
        "                                    batch_size=2048, learning_rate_init=1e-3, max_iter=50, random_state=random_seed)))\n",
        "\n",
        "# XGBoost (optional)\n",
        "if XGB_AVAILABLE:\n",
        "    models.append(('XGBoost', XGBClassifier(\n",
        "        n_estimators=500, max_depth=6, learning_rate=0.05, subsample=0.8, colsample_bytree=0.8,\n",
        "        reg_lambda=1.0, n_jobs=-1, random_state=random_seed, objective='binary:logistic', eval_metric='auc'\n",
        "    )))\n",
        "\n",
        "print(f\"Defined {len(models)} models\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Training LogisticRegression...\n",
            "\n",
            "=== LogisticRegression ===\n",
            "Accuracy: 0.9735 | Precision: 0.0566 | Recall: 0.9184 | F1: 0.1066\n",
            "ROC-AUC: 0.9576 | PR-AUC: 0.7161\n",
            "Confusion Matrix:\n",
            " [[27682   750]\n",
            " [    4    45]]\n",
            "\n",
            "LaTeX Classification Report:\n",
            "\n",
            "\\begin{table}[h!]\n",
            "\\centering\n",
            "\\begin{tabular}{lcccc}\n",
            "\\hline\n",
            "\\textbf{Class} & \\textbf{Precision} & \\textbf{Recall} & \\textbf{F1-Score} & \\textbf{Support} \\\\\n",
            "\\hline\n",
            "Non-Fraud      & 0.9999 & 0.9736 & 0.9866 & 28432 \\\\\n",
            "Fraud          & 0.0566 & 0.9184 & 0.1066 & 49 \\\\\n",
            "\\hline\n",
            "Accuracy       &        &        & 0.9735 & 28481 \\\\\n",
            "Macro Avg      & 0.5282 & 0.9460 & 0.5466 & 28481 \\\\\n",
            "Weighted Avg   & 0.9982 & 0.9735 & 0.9851 & 28481 \\\\\n",
            "\\hline\n",
            "\\end{tabular}\n",
            "\\caption{Classification Report}\n",
            "\\label{tab:classification_report}\n",
            "\\end{table}\n",
            "\n",
            "Training DecisionTree...\n",
            "\n",
            "=== DecisionTree ===\n",
            "Accuracy: 0.9973 | Precision: 0.3592 | Recall: 0.7551 | F1: 0.4868\n",
            "ROC-AUC: 0.8764 | PR-AUC: 0.2717\n",
            "Confusion Matrix:\n",
            " [[28366    66]\n",
            " [   12    37]]\n",
            "\n",
            "LaTeX Classification Report:\n",
            "\n",
            "\\begin{table}[h!]\n",
            "\\centering\n",
            "\\begin{tabular}{lcccc}\n",
            "\\hline\n",
            "\\textbf{Class} & \\textbf{Precision} & \\textbf{Recall} & \\textbf{F1-Score} & \\textbf{Support} \\\\\n",
            "\\hline\n",
            "Non-Fraud      & 0.9996 & 0.9977 & 0.9986 & 28432 \\\\\n",
            "Fraud          & 0.3592 & 0.7551 & 0.4868 & 49 \\\\\n",
            "\\hline\n",
            "Accuracy       &        &        & 0.9973 & 28481 \\\\\n",
            "Macro Avg      & 0.6794 & 0.8764 & 0.7427 & 28481 \\\\\n",
            "Weighted Avg   & 0.9985 & 0.9973 & 0.9977 & 28481 \\\\\n",
            "\\hline\n",
            "\\end{tabular}\n",
            "\\caption{Classification Report}\n",
            "\\label{tab:classification_report}\n",
            "\\end{table}\n",
            "\n",
            "Training RandomForest...\n",
            "\n",
            "=== RandomForest ===\n",
            "Accuracy: 0.9995 | Precision: 0.9091 | Recall: 0.8163 | F1: 0.8602\n",
            "ROC-AUC: 0.9527 | PR-AUC: 0.8740\n",
            "Confusion Matrix:\n",
            " [[28428     4]\n",
            " [    9    40]]\n",
            "\n",
            "LaTeX Classification Report:\n",
            "\n",
            "\\begin{table}[h!]\n",
            "\\centering\n",
            "\\begin{tabular}{lcccc}\n",
            "\\hline\n",
            "\\textbf{Class} & \\textbf{Precision} & \\textbf{Recall} & \\textbf{F1-Score} & \\textbf{Support} \\\\\n",
            "\\hline\n",
            "Non-Fraud      & 0.9997 & 0.9999 & 0.9998 & 28432 \\\\\n",
            "Fraud          & 0.9091 & 0.8163 & 0.8602 & 49 \\\\\n",
            "\\hline\n",
            "Accuracy       &        &        & 0.9995 & 28481 \\\\\n",
            "Macro Avg      & 0.9544 & 0.9081 & 0.9300 & 28481 \\\\\n",
            "Weighted Avg   & 0.9995 & 0.9995 & 0.9995 & 28481 \\\\\n",
            "\\hline\n",
            "\\end{tabular}\n",
            "\\caption{Classification Report}\n",
            "\\label{tab:classification_report}\n",
            "\\end{table}\n",
            "\n",
            "Training GradientBoosting...\n",
            "\n",
            "=== GradientBoosting ===\n",
            "Accuracy: 0.9916 | Precision: 0.1588 | Recall: 0.8980 | F1: 0.2699\n",
            "ROC-AUC: 0.9721 | PR-AUC: 0.6760\n",
            "Confusion Matrix:\n",
            " [[28199   233]\n",
            " [    5    44]]\n",
            "\n",
            "LaTeX Classification Report:\n",
            "\n",
            "\\begin{table}[h!]\n",
            "\\centering\n",
            "\\begin{tabular}{lcccc}\n",
            "\\hline\n",
            "\\textbf{Class} & \\textbf{Precision} & \\textbf{Recall} & \\textbf{F1-Score} & \\textbf{Support} \\\\\n",
            "\\hline\n",
            "Non-Fraud      & 0.9998 & 0.9918 & 0.9958 & 28432 \\\\\n",
            "Fraud          & 0.1588 & 0.8980 & 0.2699 & 49 \\\\\n",
            "\\hline\n",
            "Accuracy       &        &        & 0.9916 & 28481 \\\\\n",
            "Macro Avg      & 0.5793 & 0.9449 & 0.6329 & 28481 \\\\\n",
            "Weighted Avg   & 0.9984 & 0.9916 & 0.9945 & 28481 \\\\\n",
            "\\hline\n",
            "\\end{tabular}\n",
            "\\caption{Classification Report}\n",
            "\\label{tab:classification_report}\n",
            "\\end{table}\n",
            "\n",
            "Training LinearSVM...\n",
            "\n",
            "=== LinearSVM ===\n",
            "Accuracy: 0.9778 | Precision: 0.0669 | Recall: 0.9184 | F1: 0.1247\n",
            "ROC-AUC: 0.9637 | PR-AUC: 0.7386\n",
            "Confusion Matrix:\n",
            " [[27804   628]\n",
            " [    4    45]]\n",
            "\n",
            "LaTeX Classification Report:\n",
            "\n",
            "\\begin{table}[h!]\n",
            "\\centering\n",
            "\\begin{tabular}{lcccc}\n",
            "\\hline\n",
            "\\textbf{Class} & \\textbf{Precision} & \\textbf{Recall} & \\textbf{F1-Score} & \\textbf{Support} \\\\\n",
            "\\hline\n",
            "Non-Fraud      & 0.9999 & 0.9779 & 0.9888 & 28432 \\\\\n",
            "Fraud          & 0.0669 & 0.9184 & 0.1247 & 49 \\\\\n",
            "\\hline\n",
            "Accuracy       &        &        & 0.9778 & 28481 \\\\\n",
            "Macro Avg      & 0.5334 & 0.9481 & 0.5567 & 28481 \\\\\n",
            "Weighted Avg   & 0.9983 & 0.9778 & 0.9873 & 28481 \\\\\n",
            "\\hline\n",
            "\\end{tabular}\n",
            "\\caption{Classification Report}\n",
            "\\label{tab:classification_report}\n",
            "\\end{table}\n",
            "\n",
            "Training MLP...\n",
            "\n",
            "=== MLP ===\n",
            "Accuracy: 0.9993 | Precision: 0.7885 | Recall: 0.8367 | F1: 0.8119\n",
            "ROC-AUC: 0.9580 | PR-AUC: 0.8184\n",
            "Confusion Matrix:\n",
            " [[28421    11]\n",
            " [    8    41]]\n",
            "\n",
            "LaTeX Classification Report:\n",
            "\n",
            "\\begin{table}[h!]\n",
            "\\centering\n",
            "\\begin{tabular}{lcccc}\n",
            "\\hline\n",
            "\\textbf{Class} & \\textbf{Precision} & \\textbf{Recall} & \\textbf{F1-Score} & \\textbf{Support} \\\\\n",
            "\\hline\n",
            "Non-Fraud      & 0.9997 & 0.9996 & 0.9997 & 28432 \\\\\n",
            "Fraud          & 0.7885 & 0.8367 & 0.8119 & 49 \\\\\n",
            "\\hline\n",
            "Accuracy       &        &        & 0.9993 & 28481 \\\\\n",
            "Macro Avg      & 0.8941 & 0.9182 & 0.9058 & 28481 \\\\\n",
            "Weighted Avg   & 0.9994 & 0.9993 & 0.9993 & 28481 \\\\\n",
            "\\hline\n",
            "\\end{tabular}\n",
            "\\caption{Classification Report}\n",
            "\\label{tab:classification_report}\n",
            "\\end{table}\n",
            "\n",
            "=== Summary (sorted by PR-AUC) ===\n",
            "                model  accuracy  precision    recall        f1   roc_auc  \\\n",
            "2        RandomForest  0.999544   0.909091  0.816327  0.860215  0.952731   \n",
            "5                 MLP  0.999333   0.788462  0.836735  0.811881  0.958037   \n",
            "4           LinearSVM  0.977810   0.066865  0.918367  0.124654  0.963737   \n",
            "0  LogisticRegression  0.973526   0.056604  0.918367  0.106635  0.957580   \n",
            "3    GradientBoosting  0.991644   0.158845  0.897959  0.269939  0.972096   \n",
            "1        DecisionTree  0.997261   0.359223  0.755102  0.486842  0.876390   \n",
            "\n",
            "     pr_auc  \n",
            "2  0.873982  \n",
            "5  0.818411  \n",
            "4  0.738636  \n",
            "0  0.716127  \n",
            "3  0.676006  \n",
            "1  0.271672  \n"
          ]
        }
      ],
      "source": [
        "# Train and evaluate\n",
        "results = []\n",
        "\n",
        "for name, model in models:\n",
        "    print(f\"\\nTraining {name}...\")\n",
        "    model.fit(X_train, y_train)\n",
        "    metrics = evaluate_classifier(model, X_test, y_test, name)\n",
        "    results.append(metrics)\n",
        "\n",
        "# Create summary DataFrame\n",
        "results_df = pd.DataFrame([{k: v for k, v in r.items() if k not in ['confusion_matrix']} for r in results])\n",
        "results_df = results_df.sort_values(by='pr_auc', ascending=False)\n",
        "print('\\n=== Summary (sorted by PR-AUC) ===')\n",
        "print(results_df[['model','accuracy','precision','recall','f1','roc_auc','pr_auc']])\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
